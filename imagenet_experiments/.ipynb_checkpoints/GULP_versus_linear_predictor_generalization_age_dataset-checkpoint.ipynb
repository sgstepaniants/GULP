{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.cluster.hierarchy import dendrogram, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "# import umap\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from distance_functions import *\n",
    "import scipy.stats\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load UTK face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: This notebook will generate about 5GB of saved network representations in the utk_face_data directory.\n"
     ]
    }
   ],
   "source": [
    "print('WARNING: This notebook will generate about 5GB of saved network representations in the utk_face_data directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utk_face_data_root = './utk_face_data/'\n",
    "utk_folder_src = utk_face_data_root + \"UTKFace/\"\n",
    "utk_folder_classes = utk_face_data_root + \"UTKFace_by_class/\"\n",
    "if not os.path.isdir(utk_folder_src):\n",
    "    print('You must download UTKFace data')\n",
    "    print('1. Download the UTKFace dataset file: UTKFace.tar.gz from https://drive.google.com/drive/folders/0BxYys69jI14kU0I1YUQyY1ZDRUE?resourcekey=0-01Pth1hq20K4kuGVkp3oBw')\n",
    "    print(f'2. Unzip it in the folder {utk_face_data_root} with the command \"tar -xf UTKFace.tar.gz\"')\n",
    "    assert(False)\n",
    "    \n",
    "filenames = os.listdir(utk_folder_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTKFace class-sorted file structure already constructed\n"
     ]
    }
   ],
   "source": [
    "# shutil.rmtree(utk_folder_classes)\n",
    "\n",
    "## Sort the UTKFace dataset by class\n",
    "if not os.path.isdir(utk_folder_classes):\n",
    "    print('Converting UTKFace to class-sorted file structure')\n",
    "    os.mkdir(utk_folder_classes)\n",
    "    for filename in tqdm.tqdm(filenames):\n",
    "#         print(filename)\n",
    "        age = int(filename.split('_')[0])\n",
    "        new_age_folder = utk_folder_classes + str(age) + '/'\n",
    "        if not os.path.isdir(new_age_folder):\n",
    "            os.mkdir(new_age_folder)\n",
    "        shutil.copyfile(utk_folder_src + filename, new_age_folder + filename)\n",
    "else:\n",
    "    print('UTKFace class-sorted file structure already constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_names ['mobilenet_v2_pretrained', 'regnet_x_8gf_pretrained', 'vgg16_pretrained', 'efficientnet_b7_pretrained', 'regnet_x_400mf_pretrained', 'mobilenet_v3_large_pretrained', 'efficientnet_b5_pretrained', 'regnet_y_8gf_pretrained', 'efficientnet_b6_pretrained', 'regnet_x_3_2gf_pretrained', 'regnet_x_1_6gf_pretrained', 'regnet_x_32gf_pretrained', 'convnext_base_pretrained', 'convnext_tiny_pretrained', 'inception_pretrained', 'convnext_small_pretrained', 'resnext50_32x4d_pretrained', 'efficientnet_b1_pretrained', 'efficientnet_b3_pretrained', 'efficientnet_b4_pretrained', 'regnet_y_32gf_pretrained', 'efficientnet_b0_pretrained', 'convnext_large_pretrained', 'wide_resnet50_2_pretrained', 'regnet_y_16gf_pretrained', 'mobilenet_v3_small_pretrained', 'regnet_y_800mf_pretrained', 'regnet_x_800mf_pretrained', 'regnet_y_3_2gf_pretrained', 'regnet_y_1_6gf_pretrained', 'efficientnet_b2_pretrained', 'regnet_x_16gf_pretrained', 'resnet18_pretrained', 'mnasnet_pretrained', 'alexnet_pretrained', 'regnet_y_400mf_pretrained', 'googlenet_pretrained']\n",
      "number of models 37\n"
     ]
    }
   ],
   "source": [
    "utk_dataset = torchvision.datasets.ImageFolder(utk_folder_classes)\n",
    "\n",
    "n = 20000\n",
    "\n",
    "mode = \"eval\"\n",
    "\n",
    "reps_folder = f\"reps/utk_face/{n}_{mode}/\"\n",
    "if not os.path.exists(reps_folder):\n",
    "    os.makedirs(reps_folder)\n",
    "\n",
    "model_names = []\n",
    "file_names = os.listdir(\"models\")\n",
    "for filename in file_names:\n",
    "    if filename.endswith(\".pth\"):\n",
    "        model_names.append(filename[:-4])\n",
    "\n",
    "model_names = [x for x in model_names if 'untrained' not in x]\n",
    "print('model_names', model_names)\n",
    "total_models = len(model_names)\n",
    "print('number of models', total_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Resize(224), normalize])\n",
    "dataset = torchvision.datasets.ImageFolder(utk_folder_classes,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Re-organize the UTK face dataset so that we can access it faster, and compute representations\n",
    "\n",
    "temp_reps_folder = utk_face_data_root + 'temp_reps_utkface_noshuffle/'\n",
    "\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "stride = 200\n",
    "\n",
    "if not os.path.isdir(temp_reps_folder):\n",
    "    os.mkdir(temp_reps_folder)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=stride, shuffle=False, num_workers=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        iter_num = 0\n",
    "        for batch_data, batch_labels in loader:\n",
    "            print('iter_num',iter_num)\n",
    "\n",
    "            curr_batch_folder = temp_reps_folder + str(iter_num) + '/'\n",
    "            if not os.path.isdir(curr_batch_folder):\n",
    "                os.mkdir(curr_batch_folder)\n",
    "\n",
    "            batch_data = batch_data.to(device)\n",
    "\n",
    "            batch_names = model_names\n",
    "\n",
    "            pbar = tqdm.tqdm(batch_names)\n",
    "            for model_name in pbar:\n",
    "                pbar.set_description(f\"Processing {model_name}\")\n",
    "                pickle_file_name = curr_batch_folder + model_name + '.pkl'\n",
    "                if os.path.exists(pickle_file_name):\n",
    "                    continue\n",
    "\n",
    "\n",
    "                model = torch.load(f\"models/{model_name}.pth\")\n",
    "                model = model.to(device)\n",
    "                if mode == \"eval\":\n",
    "                    model.eval()\n",
    "\n",
    "                activation = {}\n",
    "                def get_activation(name):\n",
    "                    def hook(model, input, output):\n",
    "                        activation[name] = output.detach()\n",
    "                    return hook\n",
    "\n",
    "                try:\n",
    "                    model.classifier[-2].register_forward_hook(get_activation(\"pre_classifier\"))\n",
    "                except:\n",
    "                    list(model.children())[-2].register_forward_hook(get_activation(\"pre_classifier\"))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model(batch_data)\n",
    "                    curr_embed = activation[\"pre_classifier\"].flatten(start_dim=1)\n",
    "    #                 print(curr_embed.shape)\n",
    "\n",
    "                pickle.dump(curr_embed.cpu(), open(pickle_file_name, 'wb'))\n",
    "                model = None\n",
    "                gc.collect()\n",
    "\n",
    "            iter_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reps/utk_face/20000_eval/\n",
      "n 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 6770.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v2_pretrained\n",
      "Already exists\n",
      "regnet_x_8gf_pretrained\n",
      "Already exists\n",
      "vgg16_pretrained\n",
      "Already exists\n",
      "efficientnet_b7_pretrained\n",
      "Already exists\n",
      "regnet_x_400mf_pretrained\n",
      "Already exists\n",
      "mobilenet_v3_large_pretrained\n",
      "Already exists\n",
      "efficientnet_b5_pretrained\n",
      "Already exists\n",
      "regnet_y_8gf_pretrained\n",
      "Already exists\n",
      "efficientnet_b6_pretrained\n",
      "Already exists\n",
      "regnet_x_3_2gf_pretrained\n",
      "Already exists\n",
      "regnet_x_1_6gf_pretrained\n",
      "Already exists\n",
      "regnet_x_32gf_pretrained\n",
      "Already exists\n",
      "convnext_base_pretrained\n",
      "Already exists\n",
      "convnext_tiny_pretrained\n",
      "Already exists\n",
      "inception_pretrained\n",
      "Already exists\n",
      "convnext_small_pretrained\n",
      "Already exists\n",
      "resnext50_32x4d_pretrained\n",
      "Already exists\n",
      "efficientnet_b1_pretrained\n",
      "Already exists\n",
      "efficientnet_b3_pretrained\n",
      "Already exists\n",
      "efficientnet_b4_pretrained\n",
      "Already exists\n",
      "regnet_y_32gf_pretrained\n",
      "Already exists\n",
      "efficientnet_b0_pretrained\n",
      "Already exists\n",
      "convnext_large_pretrained\n",
      "Already exists\n",
      "wide_resnet50_2_pretrained\n",
      "Already exists\n",
      "regnet_y_16gf_pretrained\n",
      "Already exists\n",
      "mobilenet_v3_small_pretrained\n",
      "Already exists\n",
      "regnet_y_800mf_pretrained\n",
      "Already exists\n",
      "regnet_x_800mf_pretrained\n",
      "Already exists\n",
      "regnet_y_3_2gf_pretrained\n",
      "Already exists\n",
      "regnet_y_1_6gf_pretrained\n",
      "Already exists\n",
      "efficientnet_b2_pretrained\n",
      "Already exists\n",
      "regnet_x_16gf_pretrained\n",
      "Already exists\n",
      "resnet18_pretrained\n",
      "Already exists\n",
      "mnasnet_pretrained\n",
      "Already exists\n",
      "alexnet_pretrained\n",
      "Already exists\n",
      "regnet_y_400mf_pretrained\n",
      "Already exists\n",
      "googlenet_pretrained\n",
      "Already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(reps_folder)\n",
    "\n",
    "print('n',n)\n",
    "\n",
    "for model_name in tqdm.tqdm(model_names):\n",
    "    print(model_name)\n",
    "    model_rep_file = reps_folder + model_name + '.pkl'\n",
    "    if os.path.isfile(model_rep_file):\n",
    "        print('Already exists')\n",
    "        continue\n",
    "    for iter_num in range(n // stride):\n",
    "            \n",
    "        curr_file = temp_reps_folder + str(iter_num) + '/' + model_name + '.pkl'\n",
    "        if not os.path.isfile(curr_file):\n",
    "            assert(False)\n",
    "#         print(curr_file)\n",
    "        temp_embed = pickle.load(open(curr_file, 'rb'))\n",
    "        \n",
    "        if iter_num == 0:\n",
    "            d = temp_embed.shape[1]\n",
    "            curr_embed = torch.zeros((n, d))\n",
    "        \n",
    "        assert(temp_embed.shape[0] == stride)\n",
    "        assert(temp_embed.shape[1] == d)\n",
    "        curr_embed[iter_num * stride:(iter_num+1)*stride, :] = temp_embed\n",
    "    print('saving')\n",
    "    pickle.dump(curr_embed, open(model_rep_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15845, 10294, 19532,  ...,  2032, 12413,  5718])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "perm = torch.randperm(20000)\n",
    "print(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_labels = []\n",
    "for i in range(13000):\n",
    "    filename = dataset.imgs[perm[i]][0]\n",
    "    age_labels.append(int(filename.split('/')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reps/utk_face/20000_eval/\n",
      "n 20000\n",
      "mobilenet_v2_pretrained\n",
      "regnet_x_8gf_pretrained\n",
      "vgg16_pretrained\n",
      "efficientnet_b7_pretrained\n",
      "regnet_x_400mf_pretrained\n",
      "mobilenet_v3_large_pretrained\n",
      "efficientnet_b5_pretrained\n",
      "regnet_y_8gf_pretrained\n",
      "efficientnet_b6_pretrained\n",
      "regnet_x_3_2gf_pretrained\n",
      "regnet_x_1_6gf_pretrained\n",
      "regnet_x_32gf_pretrained\n",
      "convnext_base_pretrained\n",
      "convnext_tiny_pretrained\n",
      "inception_pretrained\n",
      "convnext_small_pretrained\n",
      "resnext50_32x4d_pretrained\n",
      "efficientnet_b1_pretrained\n",
      "efficientnet_b3_pretrained\n",
      "efficientnet_b4_pretrained\n",
      "regnet_y_32gf_pretrained\n",
      "efficientnet_b0_pretrained\n",
      "convnext_large_pretrained\n",
      "wide_resnet50_2_pretrained\n",
      "regnet_y_16gf_pretrained\n",
      "mobilenet_v3_small_pretrained\n",
      "regnet_y_800mf_pretrained\n",
      "regnet_x_800mf_pretrained\n",
      "regnet_y_3_2gf_pretrained\n",
      "regnet_y_1_6gf_pretrained\n",
      "efficientnet_b2_pretrained\n",
      "regnet_x_16gf_pretrained\n",
      "resnet18_pretrained\n",
      "mnasnet_pretrained\n",
      "alexnet_pretrained\n",
      "regnet_y_400mf_pretrained\n",
      "googlenet_pretrained\n"
     ]
    }
   ],
   "source": [
    "reps_folder = 'reps/utk_face/20000_eval/'\n",
    "print(reps_folder)\n",
    "reps_folder_13000 = 'reps/utk_face/13000_eval/'\n",
    "\n",
    "print('n',n)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    model_rep_file = reps_folder + model_name + '.pkl'\n",
    "    new_file = reps_folder_13000 + model_name + '.pkl'\n",
    "    if os.path.isfile(model_rep_file):\n",
    "        rep = pickle.load(open(model_rep_file, 'rb'))\n",
    "        rep = rep[perm[0:13000],:]\n",
    "        if not os.path.isfile(new_file):\n",
    "            pickle.dump(rep, open(new_file, 'wb'))\n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load distances and representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reps/utk_face/20000_eval/\n",
      "reps/utk_face/13000_eval\n"
     ]
    }
   ],
   "source": [
    "print(reps_folder)\n",
    "reps_folder = 'reps/utk_face/13000_eval'\n",
    "print(reps_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_folder = \"distances/utk_face/pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ImageNet distances between train datasets\n",
    "filenames = os.listdir(distances_folder)\n",
    "distnames = []\n",
    "for filename in filenames:\n",
    "    if filename.endswith(\"npy\"):\n",
    "        distnames.append(filename[:-4])\n",
    "distnames = np.sort(distnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lin_cka_dist' 'lin_cka_prime_dist' 'mean_cca_e2e' 'mean_sq_cca_e2e'\n",
      " 'predictor_dist_0.0' 'predictor_dist_0.0001' 'predictor_dist_0.001'\n",
      " 'predictor_dist_0.01' 'predictor_dist_0.1' 'predictor_dist_1.0'\n",
      " 'predictor_dist_10.0' 'predictor_dist_100.0' 'predictor_dist_1000.0'\n",
      " 'predictor_dist_10000.0' 'predictor_dist_1e-05' 'predictor_dist_1e-06'\n",
      " 'predictor_dist_1e-07' 'predictor_dist_1e-08' 'predictor_dist_1e-09'\n",
      " 'predictor_dist_1e-10' 'predictor_dist_1e-11' 'predictor_dist_1e-12'\n",
      " 'predictor_dist_1e-13' 'predictor_dist_1e-14' 'predictor_dist_1e-15'\n",
      " 'predictor_dist_1e-16' 'predictor_dist_1e-17' 'predictor_dist_1e-18'\n",
      " 'predictor_dist_1e-19' 'predictor_dist_1e-20' 'procrustes'\n",
      " 'pwcca_dist_e2e']\n"
     ]
    }
   ],
   "source": [
    "print(distnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.load(f\"{distances_folder}/stats.npz\")\n",
    "model_names = stats[\"model_names\"]\n",
    "total_models = len(model_names)\n",
    "dist_pairs_saved = stats[\"dist_pairs_saved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Check all distance pairs are saved. The output should be all-zeros\n",
    "print(np.arange(total_models) - np.sum(dist_pairs_saved, axis=0))\n",
    "print(np.arange(total_models) - np.flip(np.sum(dist_pairs_saved, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet_pretrained' 'convnext_base_pretrained'\n",
      " 'convnext_large_pretrained' 'convnext_small_pretrained'\n",
      " 'convnext_tiny_pretrained' 'efficientnet_b0_pretrained'\n",
      " 'efficientnet_b1_pretrained' 'efficientnet_b2_pretrained'\n",
      " 'efficientnet_b3_pretrained' 'efficientnet_b4_pretrained'\n",
      " 'efficientnet_b5_pretrained' 'efficientnet_b6_pretrained'\n",
      " 'efficientnet_b7_pretrained' 'googlenet_pretrained'\n",
      " 'inception_pretrained' 'mnasnet_pretrained' 'mobilenet_v2_pretrained'\n",
      " 'mobilenet_v3_large_pretrained' 'mobilenet_v3_small_pretrained'\n",
      " 'regnet_x_16gf_pretrained' 'regnet_x_1_6gf_pretrained'\n",
      " 'regnet_x_32gf_pretrained' 'regnet_x_3_2gf_pretrained'\n",
      " 'regnet_x_400mf_pretrained' 'regnet_x_800mf_pretrained'\n",
      " 'regnet_x_8gf_pretrained' 'regnet_y_16gf_pretrained'\n",
      " 'regnet_y_1_6gf_pretrained' 'regnet_y_32gf_pretrained'\n",
      " 'regnet_y_3_2gf_pretrained' 'regnet_y_400mf_pretrained'\n",
      " 'regnet_y_800mf_pretrained' 'regnet_y_8gf_pretrained'\n",
      " 'resnet18_pretrained' 'resnext50_32x4d_pretrained' 'vgg16_pretrained'\n",
      " 'wide_resnet50_2_pretrained']\n"
     ]
    }
   ],
   "source": [
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_cka_dist\n",
      "lin_cka_prime_dist\n",
      "mean_cca_e2e\n",
      "mean_sq_cca_e2e\n",
      "predictor_dist_0.0\n",
      "predictor_dist_0.0001\n",
      "predictor_dist_0.001\n",
      "predictor_dist_0.01\n",
      "predictor_dist_0.1\n",
      "predictor_dist_1.0\n",
      "predictor_dist_10.0\n",
      "predictor_dist_100.0\n",
      "predictor_dist_1000.0\n",
      "predictor_dist_10000.0\n",
      "predictor_dist_1e-05\n",
      "predictor_dist_1e-06\n",
      "predictor_dist_1e-07\n",
      "predictor_dist_1e-08\n",
      "predictor_dist_1e-09\n",
      "predictor_dist_1e-10\n",
      "predictor_dist_1e-11\n",
      "predictor_dist_1e-12\n",
      "predictor_dist_1e-13\n",
      "predictor_dist_1e-14\n",
      "predictor_dist_1e-15\n",
      "predictor_dist_1e-16\n",
      "predictor_dist_1e-17\n",
      "predictor_dist_1e-18\n",
      "predictor_dist_1e-19\n",
      "predictor_dist_1e-20\n",
      "procrustes\n",
      "pwcca_dist_e2e\n"
     ]
    }
   ],
   "source": [
    "distances = {}\n",
    "def symmetrize(A):\n",
    "    n = A.shape[0]\n",
    "    B = A.copy()\n",
    "    B[np.tril_indices(n)] = B.T[np.tril_indices(n)]\n",
    "    return B\n",
    "for distname in distnames:\n",
    "    print(distname)\n",
    "    curr_dist = np.load(f\"{distances_folder}/{distname}.npy\")  \n",
    "    distances[distname] = symmetrize(curr_dist)\n",
    "#     print(distances[distname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet_pretrained\n",
      "convnext_base_pretrained\n",
      "convnext_large_pretrained\n",
      "convnext_small_pretrained\n",
      "convnext_tiny_pretrained\n",
      "efficientnet_b0_pretrained\n",
      "efficientnet_b1_pretrained\n",
      "efficientnet_b2_pretrained\n",
      "efficientnet_b3_pretrained\n",
      "efficientnet_b4_pretrained\n",
      "efficientnet_b5_pretrained\n",
      "efficientnet_b6_pretrained\n",
      "efficientnet_b7_pretrained\n",
      "googlenet_pretrained\n",
      "inception_pretrained\n",
      "mnasnet_pretrained\n",
      "mobilenet_v2_pretrained\n",
      "mobilenet_v3_large_pretrained\n",
      "mobilenet_v3_small_pretrained\n",
      "regnet_x_16gf_pretrained\n",
      "regnet_x_1_6gf_pretrained\n",
      "regnet_x_32gf_pretrained\n",
      "regnet_x_3_2gf_pretrained\n",
      "regnet_x_400mf_pretrained\n",
      "regnet_x_800mf_pretrained\n",
      "regnet_x_8gf_pretrained\n",
      "regnet_y_16gf_pretrained\n",
      "regnet_y_1_6gf_pretrained\n",
      "regnet_y_32gf_pretrained\n",
      "regnet_y_3_2gf_pretrained\n",
      "regnet_y_400mf_pretrained\n",
      "regnet_y_800mf_pretrained\n",
      "regnet_y_8gf_pretrained\n",
      "resnet18_pretrained\n",
      "resnext50_32x4d_pretrained\n",
      "vgg16_pretrained\n",
      "wide_resnet50_2_pretrained\n"
     ]
    }
   ],
   "source": [
    "tot_reps_folder = 'reps/utk_face/13000_eval/'\n",
    "n1 = 10000\n",
    "n2 = 3000\n",
    "\n",
    "# Load ImageNet representations\n",
    "reps_train = {} # Train dataset\n",
    "reps_val = {} # Validation dataset\n",
    "try:\n",
    "    for model_name in model_names:\n",
    "        print(model_name)\n",
    "        tot_rep = pickle.load(open(tot_reps_folder + model_name + '.pkl', 'rb'))\n",
    "        rep1 = np.asarray(tot_rep[0:n1,:])\n",
    "        rep1 = rep1.T\n",
    "        rep2 = np.asarray(tot_rep[n1:n1+n2,:])\n",
    "        rep2 = rep2.T\n",
    "#         print(tot_rep.shape)\n",
    "#         print(rep1.shape)\n",
    "#         print(rep2.shape)\n",
    "        \n",
    "        # center and normalize\n",
    "        rep1 = rep1 - rep1.mean(axis=1, keepdims=True)\n",
    "        rep1 = rep1 / np.linalg.norm(rep1)\n",
    "        rep1 = rep1 * np.sqrt(rep1.shape[1])\n",
    "        reps_train[model_name] = rep1\n",
    "\n",
    "        # center and normalize\n",
    "        rep2 = rep2 - rep2.mean(axis=1, keepdims=True)\n",
    "        rep2 = rep2 / np.linalg.norm(rep2)\n",
    "        rep2 = rep2 * np.sqrt(rep2.shape[1])\n",
    "        reps_val[model_name] = rep2\n",
    "        \n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print('WARNING: IN ORDER TO RUN THIS CODE, THE UTK_FACE REPRESENTATIONS MUST COMPUTED. SEE README.')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict generalization, y is age label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pred(y, lmbda, reps):\n",
    "    # ridge regression \n",
    "    # assume reps is dimension x number datapoints \n",
    "    rep_dim = reps.shape[0]\n",
    "    numpts = reps.shape[1]\n",
    "    \n",
    "    return np.linalg.solve((lmbda*np.eye(rep_dim) + (reps @ reps.T) / numpts), reps@y) / numpts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (335449039.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [22]\u001b[0;36m\u001b[0m\n\u001b[0;31m    cp2 = preds[model_names[ind2]].T @ reps_val[model_names[ind2]]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_collected_correlations(lmbda, numtrials=50, numtrainsamples=10000):\n",
    "    collected_correlations = []\n",
    "\n",
    "    labels = []\n",
    "    for ky,val in distances.items():\n",
    "        if ky != 'predictor_dist_range':\n",
    "            labels.append(ky)\n",
    "#     print(labels)\n",
    "\n",
    "    def flatten_upper_right_triangle(curr_mat):\n",
    "        cv = []\n",
    "        assert(curr_mat.shape[0] == curr_mat.shape[1])\n",
    "        assert(curr_mat.shape[0] == len(model_names))\n",
    "        for i in range(len(model_names)-1):\n",
    "            for j in range(i+1,len(model_names)):\n",
    "                cv.append(curr_mat[i,j])\n",
    "        cv = np.asarray(cv)\n",
    "        return cv\n",
    "\n",
    "    dist_vecs = {}\n",
    "\n",
    "    for distname in labels:\n",
    "        dist_vecs[distname] = flatten_upper_right_triangle(distances[distname])\n",
    "\n",
    "    for tri in range(numtrials):\n",
    "        print(f'Trial {tri}')\n",
    "\n",
    "        print('USING AGE LABELS')\n",
    "        print('centering')\n",
    "        avg_age = np.mean(age_labels)\n",
    "        y = np.asarray(age_labels[0:numtrainsamples] - avg_age)\n",
    "\n",
    "        preds = {}\n",
    "        for model_name in model_names:\n",
    "            preds[model_name] = find_best_pred(y, lmbda, reps_train[model_name][:,0:numtrainsamples])\n",
    "\n",
    "        # For each pair, compute the squared distance between predictions, averaged over test instances \n",
    "        errs = np.zeros((len(model_names), len(model_names)))\n",
    "        truelabels = np.asarray(age_labels[10000:13000]) - avg_age\n",
    "        for ind1 in tqdm.tqdm(range(0, len(model_names)-1)):\n",
    "            for ind2 in range(ind1+1, len(model_names)):\n",
    "                cp1 = preds[model_names[ind1]].T @ reps_val[model_names[ind1]]\n",
    "#                 if ind2 == ind1+1:\n",
    "#                     print('stddev error',math.sqrt(np.sum((cp1 - truelabels) ** 2) / len(cp1)))\n",
    "#                     print('baseline',math.sqrt(np.sum((truelabels) ** 2) / len(cp1)))\n",
    "                cp2 = preds[model_names[ind2]].T @ reps_val[model_names[ind2]]\n",
    "                errs[ind1, ind2] = np.linalg.norm(cp1 - cp2)\n",
    "                errs[ind2, ind1] = errs[ind1, ind2]\n",
    "        print(errs)\n",
    "        err_vec = flatten_upper_right_triangle(errs)\n",
    "\n",
    "        correlations = []\n",
    "\n",
    "        for distname in labels:\n",
    "            val = scipy.stats.spearmanr(err_vec, dist_vecs[distname]).correlation\n",
    "            correlations.append(val)\n",
    "\n",
    "        collected_correlations.append(correlations)\n",
    "        \n",
    "    return labels, collected_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_train['alexnet_pretrained'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_dict = {'lin_cka_dist': 'CKA', 'mean_sq_cca_e2e' : 'CCA', 'pwcca_dist_e2e': 'PWCCA', 'procrustes': 'Procrustes'}\n",
    "\n",
    "label_names_dict['predictor_dist_0.0'] = 'GULP, $\\lambda = 0$'\n",
    "label_names_dict['predictor_dist_1e-07'] = 'GULP, $\\lambda = 10^{-7}$'\n",
    "label_names_dict['predictor_dist_1e-06'] = 'GULP, $\\lambda = 10^{-6}$'\n",
    "label_names_dict['predictor_dist_1e-05'] = 'GULP, $\\lambda = 10^{-5}$'\n",
    "label_names_dict['predictor_dist_0.0001'] = 'GULP, $\\lambda = 10^{-4}$'\n",
    "label_names_dict['predictor_dist_0.001'] = 'GULP, $\\lambda = 10^{-3}$'\n",
    "label_names_dict['predictor_dist_0.01'] = 'GULP, $\\lambda = 10^{-2}$'\n",
    "label_names_dict['predictor_dist_0.1'] = 'GULP, $\\lambda = 10^{-1}$'\n",
    "label_names_dict['predictor_dist_1.0'] = 'GULP, $\\lambda = 1$'\n",
    "label_names_dict['predictor_dist_10.0'] = 'GULP, $\\lambda = 10$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmbda_vals = [1e-6,1e-4,1e-2,1]\n",
    "lmbda_names = [\"10^{-6}\", \"10^{-4}\", \"10^{-2}\", \"1\"]\n",
    "numtrials=1\n",
    "for lmbda_ind, lmbda in enumerate(lmbda_vals):\n",
    "    labels, collected_correlations = get_collected_correlations(lmbda, numtrials=numtrials,numtrainsamples=5000)\n",
    "    \n",
    "    std_devs = []\n",
    "    means = []\n",
    "    for i in range(len(collected_correlations[0])):\n",
    "        cvs = [collected_correlations[j][i] for j in range(len(collected_correlations))]\n",
    "        if numtrials == 1:\n",
    "            std_devs.append(0)\n",
    "        else:\n",
    "            std_devs.append(scipy.stats.sem(cvs))\n",
    "        means.append(np.mean(cvs))\n",
    "    std_devs = np.array(std_devs)\n",
    "    means = np.array(means)\n",
    "#     print(std_devs)\n",
    "    subset_labels = ['lin_cka_dist', 'pwcca_dist_e2e', 'procrustes', 'mean_sq_cca_e2e', 'predictor_dist_0.0', 'predictor_dist_1e-07', 'predictor_dist_1e-06', 'predictor_dist_1e-05', 'predictor_dist_0.0001', 'predictor_dist_0.001', 'predictor_dist_0.01', 'predictor_dist_0.1', 'predictor_dist_1.0', 'predictor_dist_10.0']\n",
    "    subset_label_names = [label_names_dict[x] for x in subset_labels]\n",
    "    subset_indices = []\n",
    "    for i in range(len(subset_labels)):\n",
    "        subset_indices.append(labels.index(subset_labels[i]))\n",
    "    #     print(subset_indices)\n",
    "    lbels = labels\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(range(len(subset_labels)),means[subset_indices], yerr = std_devs[subset_indices])\n",
    "    plt.xticks(range(len(subset_labels)), labels=subset_label_names, rotation='vertical', fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylim(top=1)\n",
    "    plt.title(f'Spearman $\\\\rho$ with prediction distance for $\\\\lambda = {lmbda_names[lmbda_ind]}$', fontsize=16)\n",
    "    plt.savefig('../paper_figures/utk_generalization_lambda' + str(lmbda) + '.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
